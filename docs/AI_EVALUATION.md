# AI Evaluation & Reward Mechanism

为了让多个大模型（Gemini、Codex、Perplexity、Deepseek 等）能够在 AI-native 流程里自助迭代，我们定义了统一的评价与奖励机制。所有 Agent 在开始任何任务前，必须阅读并遵守本文件。

## 1. 目标

- **一致性**：不同大模型按照同一套标准交付产出，避免风格和质量漂移。
- **可量化**：为每次迭代生成可追踪的指标，驱动自动化调度与奖励。
- **可扩展**：指标可配置/扩充，便于引入新的工作流（数据校验、推理、部署等）。

## 2. 必读流程

1. 打开 `docs/AI_EVALUATION.md`（本文件）并确认版本号/更新时间。
2. 在 `docs/project/phrase_xxx/iteration_flow.md` 记录“已阅读评价机制”的时间戳。
3. 若本文件更新，必须同步更新 `TODOWRITE.md` 中的相关要求并广播到当前迭代笔记。

> **强制要求**：所有 Agent 在执行任何 `shell` 或 `apply_patch` 操作前，确认已完成上述步骤。违者视为流程违规，得分直接置 0。

## 3. 评价维度 & 权重

| 维度 | 描述 | 权重 | 评分方法 |
| --- | --- | --- | --- |
| **Process** | 是否遵守 prompts 记录、`AGENTS.md`、Doc 更新链条 | 20% | 满足 = 1，存在遗漏 = 0 |
| **Data Veracity** | 数据来源 ≥3 个、避免重复抓取、空值优先策略 | 20% | 0/0.5/1（无触及 = 0，部分引用 = 0.5，严格执行 = 1） |
| **Testing** | 是否执行并记录测试/校验（含 manage.py test、数据校验脚本等） | 20% | 自动化日志 + 测试结果截图/引用 |
| **Delivery Quality** | 代码/文档是否遵循目录规范、SSOT 原则、可读性 | 20% | 通过代码审查 checklist 评估 |
| **Impact** | 是否推进核心需求（界面、因子、数据管理、对话、推送）或产生自驱创新 | 20% | 迭代目标达成度，0/0.5/1 |

总评分 = Σ(维度得分 × 权重)。≥0.8 视为“奖励”，0.5~0.8 视为“可改进”，<0.5 需立刻复盘。

## 4. 奖励机制

- **奖励**：将高分迭代记录到 `x-log/rewards.log`（自动/手动），并允许下次优先领取高优任务。
- **可改进**：需在 `TODOWRITE.md` 中追加明确的整改项，并在下次迭代优先完成。
- **复盘**：触发专门会议/对话，分析失败原因（流程未遵守、测试缺失、数据错误等）。

## 5. 自动化采集计划

1. **脚本化打分**：未来将提供 `nx run score:agent --iteration=<id>`，自动读取日志/README/测试输出计算分数。
2. **数据落地**：评分结果写入 `x-data/agent_scores/<date>.json`，供可视化及策略引擎使用。
3. **触发器**：当评分 <0.5 时，自动阻塞下一步 CI/CD，提示需要人工确认。

## 6. 与需求的关系

- 满足“帮我设计好多 agent 的奖励机制”这一非功能需求。
- 连接 `agent.md`、`docs/TODOWRITE.md`、`docs/project/phrase_xxx`，形成闭环。
- 作为 ping-pong 全流程之前的必备保障，确保后续自动化有可度量的目标。

## 7. 下一步

- 将本文件链接到 `AGENTS.md`，要求所有 Agent 入口阅读。
- 在 `docs/project/phrase_1.initial_setup/plan.md` 中添加“评价机制”交付物。
- 实现打分脚本 & 日志管道（未来任务）。
